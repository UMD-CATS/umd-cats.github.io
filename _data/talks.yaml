
-
    speaker: "Magdalen Dobson"
    speaker_url: https://csd.cmu.edu/people/doctoral-student/magdalen-dobson
    venue: "IRB 4105"
    datetime: 2023-11-10 10:00

-
    title: "Building Personalized Decision Models with Federated Human Preferences"
    speaker: "Aadirupa Saha"
    speaker_url: https://aadirupa.github.io/
    venue: "Zoom"
    datetime: 2023-11-09 14:00
    abstract: |
      Customer statistics collected in several real-world systems have reflected that users often prefer eliciting their liking for a given pair of items, say (A,B), in terms of relative queries like: "Do you prefer Item A over B?", rather than their absolute counterparts: ``How much do you score items A and B on a scale of [0-10]?". Drawing inspirations, in the search for a more effective feedback collection mechanism, led to the famous formulation of Dueling Bandits (DB), which is a widely studied online learning framework for efficient information aggregation from relative/comparative feedback. However despite the novel objective, unfortunately, most of the existing DB techniques were limited only to simpler settings of finite decision spaces, and stochastic environments, which are unrealistic in practice.

      In this talk, we will start with the basic problem formulations for DB and familiarize ourselves with some of the breakthrough results. Following this, will dive deeper into a more practical framework of contextual dueling bandits (C-DB) where the goal of the learner is to make personalized predictions based on the user contexts. We will see a new algorithmic approach that can efficiently achieve the optimal O(\sqrt T) regret performance for this problem, resolving an open problem from Dudík et al. [COLT, 2015]. In the last part of the talk, we will extend the aforementioned models to a federated framework, which entails developing preference-driven prediction models for distributed environments for creating large-scale personalized systems, including recommender systems and chatbot interactions. Apart from exploiting the limited preference feedback model, the challenge lies in ensuring user privacy and reducing communication complexity in the federated setting. We will conclude the talk with some interesting open problems.
    bio: |
      Aadirupa is currently a research scientist at Apple ML research, broadly working in the area of Machine Learning theory. She did a short-term research visit at Toyota Technological Institute, Chicago (TTIC), after finishing her postdoc at Microsoft Research New York City. She obtained her Ph.D. from IISc Bangalore with Aditya Gopalan and Chiranjib Bhattacharyya.

      Research interests: Broadly anything related to designing Efficient Human Aligned Prediction Models. A few specific research areas include Online learning theory, Bandits & RL, Federated Optimization, and Differential Privacy. Of late, she has also been working on some problems at the intersection of Mechanism Design, Game Theory, and Algorithmic Fairness.

-
    speaker: "Shangdi Yu"
    speaker_url: https://yushangdi.github.io/
    venue: "IRB 4105"
    datetime: 2023-10-30 10:00

-
    speaker: "Eklavya Sharma"
    speaker_url: https://sharmaeklavya2.github.io/
    venue: "Zoom"
    datetime: 2023-10-27 10:00

-
    title: "Near-Optimal Differentially Private k-Core Decomposition"
    speaker: "George Li"
    speaker_url: https://sites.google.com/view/gzli929/home
    venue: "IRB 4105"
    datetime: 2023-10-20 10:00

-
    title: "Ex-Post Group Fairness and Individual Fairness in Ranking"
    speaker: "Sruthi Gorantla"
    speaker_url: https://sites.google.com/view/sruthigorantla/home
    venue: "IRB 3137"
    datetime: 2023-10-10 12:15
    abstract: |
      Fair ranking tasks, which ask to rank a set of items to maximize utility subject to satisfying group-fairness constraints, have gained significant interest in algorithmic fairness, information retrieval, and machine learning literature. Recent works identify uncertainty in the utilities of items as a primary cause of unfairness and propose randomized rankings that achieve ex-ante fairer exposure and better robustness than deterministic rankings. However, this still may not guarantee representation fairness to the groups ex-post. In this talk, we will first discuss algorithms to sample a random group-fair ranking from the distribution that satisfies a set of natural axioms for randomized group-fair rankings. Our problem formulation works even when there is implicit bias, incomplete relevance information, or only an ordinal ranking is available instead of relevance scores or utility values. Next, we will look at its application to efficiently train stochastic learning-to-rank algorithms via in-processing for ex-post fairness. Finally, we will discuss an efficient algorithm that samples rankings from an individually fair distribution while ensuring ex-post group fairness.

      This talk is based on joint works with Eshaan Bhansali (UC Berkeley), Amit Deshpande (Microsoft Research Bengaluru), Anand Louis (Indian Insitute of Science), and Anay Mehrotra (Yale University).
    bio: Sruthi is a final-year PhD Candidate in the department of Computer Science and Automation at the Indian Institute of Science, Bengaluru, where she is advised by Prof. Anand Louis. During her PhD, Sruthi has interned with Google Research Bengaluru in the "AI for Social Impact" Team and INRIA Saclay in the "FairPlay" Team. She was one of the recipients of the Google PhD Fellowship in the area of "Algorithms, Markets, and Optimization" for the year 2021. Her research focuses on algorithmic fairness. Her works on fairness in ranking and clustering have appeared in several peer-reviewed conferences.

-
    title: "Controlling Tail Risk in Online Ski-Rental"
    speaker: "Michael Dinitz"
    speaker_url: https://www.cs.jhu.edu/~mdinitz/
    venue: "IRB 4105"
    datetime: 2023-10-06 10:00
    abstract: |
      The classical ski-rental problem admits a textbook 2-competitive deterministic algorithm, and a simple randomized algorithm that is e/(e−1)-competitive in expectation. The randomized algorithm, while optimal in expectation, has a large variance in its performance: it has more than a 37% chance of competitive ratio exceeding 2, and a Θ(1/n) chance of the competitive ratio exceeding n.

      We ask what happens to the optimal solution if we insist that the tail risk, i.e., the chance of the competitive ratio exceeding a specific value, is bounded by some constant δ. We find that this additional modification significantly changes the structure of the optimal solution. The probability of purchasing skis on a given day becomes non-monotone, discontinuous, and grows arbitrarily quickly (for sufficiently small tail risk δ and large purchase cost n).

      Joint work with Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii
    bio: Michael Dinitz is an Associate Professor in the Department of Computer Science at Johns Hopkins University, with a secondary appointment in the Department of Applied Mathematics and Statistics.  He was previously a postdoctoral fellow at the Weizmann Institute of Science, and received his PhD from Carnegie Mellon University.  He works primarily in theoretical computer science, with a focus on approximation and graph algorithms, and occasionally with applications to computer networking, distributed computing, or machine learning. 

-
    title: "On (1+ε)-Approximate Flow Sparsifiers"
    speaker: "Zihan Tan"
    speaker_url: https://sites.google.com/view/zihantan
    venue: "IRB 4105"
    datetime: 2023-10-03 15:30
    abstract: |
      Given a large graph G with a subset |T|=k of its vertices called terminals, a quality-q flow sparsifier is a small graph G that contains T and preserves all multicommodity flows that can be routed between terminals in T, to within factor q. The problem of constructing flow sparsifiers with good (small) quality and (small) size has been a central problem in graph compression for decades.

      A natural approach of constructing O(1)-quality flow sparsifiers, which was adopted in most previous constructions, is contraction. Andoni, Krauthgamer, and Gupta constructed a sketch of size f(k, ε) that stores all feasible multicommodity flows up to factor (1+ ε), raised the question of constructing quality-(1+ ε) flow sparsifiers whose size only depends on k,\eps (but not the number of vertices in the input graph G), and proposed a contraction-based framework towards it using their sketch result. In this paper, we settle their question for contraction-based flow sparsifiers, by showing that quality-(1+\eps) contraction-based flow sparsifiers with size f(ε) exist for all 5-terminal graphs, but not all 6-terminal graphs. Our hardness result on 6-terminal graphs improves upon a recent hardness result by Krauthgamer and Mosenzon on exact (quality-1) flow sparsifiers, for contraction-based constructions. Our construction and proof utilize the notion of tight span in metric geometry.

      This talk is based on joint work with Yu Chen.
    bio: Zihan Tan is a postdoctoral associate at DIMACS, Rutgers University. Before joining DIMACS, Zihan Tan obtained his Ph.D. from the University of Chicago, where he was advised by Professor Julia Chuzhoy and Professor László Babai. He is broadly interested in theoretical computer science, with a focus on designing algorithms and proving lower bounds for combinatorial optimization problems. He is currently working on graph problems, especially on exploring the interplay between structural graph theory and graph algorithms.

-
    title: "2-Approximation for Prize-Collecting Steiner Forest"
    speaker: "Ali, Iman, Peyman, and Mohammad"
    venue: "IRB 4105"
    datetime: 2023-09-22 10:00
    abstract: Approximation algorithms for the prize-collecting Steiner forest problem (PCSF) have been a subject of research for over three decades, starting with the seminal works of Agrawal, Klein, and Ravi and Goemans and Williamson on Steiner forest and prize-collecting problems. In this paper, we propose and analyze a natural deterministic algorithm for PCSF that achieves a 2-approximate solution in polynomial time. This represents a significant improvement compared to the previously best known algorithm with a 2.54-approximation factor developed by Hajiaghayi and Jain in 2006. Furthermore, Könemann, Olver, Pashkovich, Ravi, Swamy, and Vygen have established an integrality gap of at least 9/4 for the natural LP relaxation for PCSF. However, we surpass this gap through the utilization of a combinatorial algorithm and a novel analysis technique. Since 2 is the best known approximation guarantee for Steiner forest problem, which is a special case of PCSF, our result matches this factor and closes the gap between the Steiner forest problem and its generalized version, PCSF.
    bio: Ali, Iman, Peyman, and Mohammad are second-year PhD students at the University of Maryland, College Park, advised by MohammadTaghi Hajiaghayi. Their research interests primarily lie in algorithmic graph theory and approximation algorithms.

-
    title: "On a generalization of iterated and randomized rounding"
    speaker: "Renata Valieva"
    speaker_url: https://www.cs.umd.edu/people/rvalieva
    venue: "IRB 0318 Gannon Auditorium"
    datetime: 2023-09-15 10:00
    abstract: The author gives a general method for rounding linear programs that combines the commonly used iterated rounding and randomized rounding techniques. In particular, it is shown that whenever iterated rounding can be applied to a problem with some slack, there is a randomized procedure that returns an integral solution that satisfies the guarantees of iterated rounding and also has concentration properties. The author uses this to give new results for several classic problems where iterated rounding has been useful.
    bio: Renata is a third-year PhD student at the University of Maryland, College Park, advised by Aravind Srinivasan. Her interests lie in the design and analysis of randomized algorithms, with a focus in dependent rounding and concentration inequalities.

-
    title: "Improved Bi-point Rounding Algorithms and a Golden Barrier for k-Median"
    speaker: "Kishen N Gowda"
    speaker_url: https://www.cs.umd.edu/people/kishen19
    venue: "IRB 4105"
    datetime: 2023-09-08 10:00
    abstract: |
      The current best approximation algorithms for k-median rely on first obtaining a structured fractional solution known as a bi-point solution, and then rounding it to an integer solution. We improve this second step by unifying and refining previous approaches. We describe a hierarchy of increasingly-complex partitioning schemes for the facilities, along with corresponding sets of algorithms and factor-revealing non-linear programs. We prove that the third layer of this hierarchy is a 2.613-approximation, improving upon the current best ratio of 2.675, while no layer can be proved better than 2.588 under the proposed analysis. On the negative side, we give a family of bi-point solutions which cannot be approximated better than the square root of the golden ratio, even if allowed to open k+o(k) facilities. This gives a barrier to current approaches for obtaining an approximation better than 2√φ ≈ 2.544. Altogether we reduce the approximation gap of bi-point solutions by two thirds.

      Joint work with Thomas Pensyl, Aravind Srinivasan and Khoa Trinh.
    bio: Kishen is a third-year PhD student at the University of Maryland, College Park, advised by Laxman Dhulipala and Aravind Srinivasan. His research interests broadly lie in efficient parallel graph algorithms, approximation and parameterized algorithms for problems in combinatorial optimization with a focus on algorithmic fairness.

-
    title: "Memory as a lens to understand efficient learning and optimization"
    speaker: "Vatsal Sharan"
    speaker_url: https://vatsalsharan.github.io/
    venue: "IRB 4105"
    datetime: 2023-09-05 15:30
    abstract: |
      What is the role of memory in learning and optimization? The optimal convergence rates (measures in terms of the number of oracle queries or samples needed) for various optimization problems are achieved by computationally expensive optimization techniques, such as second-order methods and cutting-plane methods. We will discuss if simpler, faster and memory-limited algorithms such as gradient descent can achieve these optimal convergence rates for the prototypical optimization problem of minimizing a convex function with access to a gradient or a stochastic gradient oracle. Our results hint at a perhaps curious dichotomy---it is not possible to significantly improve on the convergence rate of known memory efficient techniques (which are linear-memory variants of gradient descent for many of these problems) without using substantially more memory (quadratic memory for many of these problems). Therefore memory could be a useful discerning factor to provide a clear separation between 'efficient' and 'expensive' techniques. Finally, we also discuss how exploring the landscape of memory-limited optimization sheds light on new problem structures where it is possible to circumvent our lower bounds, and suggests new variants of gradient descent. 
    
      Based on joint works with Annie Marsden, Aaron Sidford, Gregory Valiant, Jonathan Kelner and Honglin Yuan.
    bio: Vatsal Sharan is an assistant professor in the CS department at the University of Southern California. He did his PhD at Stanford and a postdoc at MIT. He is interested in the foundations of machine learning, particularly in questions of computational & statistical efficiency, fairness and robustness. His work has been recognized with a best paper award at COLT (2022), an Amazon Research Award (2022) and a NSF CAREER award (2023).